# Humanoid Robot Whole-Body Control Papers

A curated list of recent papers on humanoid robot whole-body control and motion tracking.

## Papers

### 1. LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning
**Authors**: Yiyang Shao, Bike Zhang, Qiayuan Liao, Xiaoyu Huang, Yuman Gao, Yufeng Chi, Zhongyu Li, Sophia Shao, Koushil Sreenath

**Links**:
- [arXiv](https://arxiv.org/abs/2504.21738)
- [Project Page](https://langwbc.github.io/)

**Summary**: An end-to-end, language-directed policy for real-world humanoid whole-body control using reinforcement learning with policy distillation and CVAE structure.

---

### 2. HuB: Learning Extreme Humanoid Balance
**Authors**: Tong Zhang, Boyuan Zheng, Ruiqian Nai, Yingdong Hu, Yen-Jen Wang, Geng Chen, Fanqi Lin, Jiongye Li, Chuye Hong, Koushil Sreenath, Yang Gao

**Links**:
- [arXiv](https://arxiv.org/abs/2505.07294)
- [Project Page](https://hub-robot.github.io/)

**Summary**: A unified framework for humanoid control in extreme balance tasks, integrating reference motion refinement, balance-aware policy learning, and sim-to-real robustness training.

---

### 3. Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics
**Authors**: Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha

**Links**:
- [arXiv](https://arxiv.org/abs/2508.13444)

**Summary**: A low-cost pipeline leveraging motion-sensing console games (Just Dance) to evaluate whole-body robot control policies and benchmark humanoid athletic performance.

---

### 4. BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion
**Authors**: Qiayuan Liao, Takara E. Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, C. Karen Liu

**Links**:
- [arXiv](https://arxiv.org/abs/2508.08241)
- [Project Page](https://beyondmimic.github.io/)
- [GitHub](https://github.com/HybridRobotics/whole_body_trac)

**Summary**: A real-world framework for learning from human motions for versatile humanoid control via guided diffusion, enabling motion tracking and zero-shot task-specific control.

---

### 5. Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics Motions
**Authors**: Ziwen Zhuang, Hang Zhao

**Links**:
- [arXiv](https://arxiv.org/abs/2502.01465)
- [Project Page](https://project-instinct.github.io/)

**Summary**: A framework enabling humanoid robots to use all body parts for interaction with the environment, handling unpredictable contact sequences.

---

### 6. HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos
**Authors**: Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, Guanya Shi

**Links**:
- [arXiv](https://arxiv.org/abs/2509.16757)
- [Project Page](https://hdmi-humanoid.github.io/)
- [GitHub](https://github.com/LeCAR-Lab/HDMI)

**Summary**: A framework that learns whole-body humanoid-object interaction skills directly from monocular RGB videos, enabling robust loco-manipulation tasks.

---

### 7. DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion
**Authors**: Dvij Kalaria, Sudarshan S Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, S. Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Huang

**Links**:
- [arXiv](https://arxiv.org/abs/2509.14353)
- [Project Page](https://genrobo.github.io/DreamControl/)

**Summary**: A methodology using diffusion models and RL with human motion-informed priors to enable autonomous whole-body humanoid skills for scene interaction.

---

### 8. GMT: General Motion Tracking for Humanoid Whole-Body Control
**Authors**: Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, Xiaolong Wang

**Links**:
- [arXiv](https://arxiv.org/abs/2506.14770)
- [Project Page](https://gmt-humanoid.github.io/)
- [GitHub](https://github.com/zixuan417/humanoid-general-motion-tracking)

**Summary**: A general and scalable motion-tracking framework using Adaptive Sampling strategy and Motion Mixture-of-Experts architecture for diverse whole-body motions.

---

### 9. OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction
**Authors**: Lujie Yang, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, Guanya Shi

**Links**:
- [arXiv](https://arxiv.org/abs/2509.26633)
- [Project Page](https://omniretarget.github.io/)

**Summary**: An interaction-preserving data generation engine for humanoid loco-manipulation, explicitly modeling spatial and contact relationships between agent, terrain, and objects.

---

### 10. LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction
**Authors**: Haoru Xue, Xiaoyu Huang, Dantong Niu, Qiayuan Liao, Thomas Kragerud, Jan Tommy Gravdahl, Xue Bin Peng, Guanya Shi, Trevor Darrell, Koushil Sreenath, Shankar Sastry

**Links**:
- [arXiv](https://arxiv.org/abs/2506.13751)
- [Project Page](https://ember-lab-berkeley.github.io/LeVERB-Website/)

**Summary**: A hierarchical latent instruction-following framework for humanoid vision-language whole-body control, learning latent action vocabulary from synthetic demonstrations.

---

### 11. Visual Imitation Enables Contextual Humanoid Control (VideoMimic)
**Authors**: Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa

**Links**:
- [arXiv](https://arxiv.org/abs/2505.03729)
- [Project Page](https://www.videomimic.net/)
- [GitHub](https://github.com/hongsukchoi/VideoMimic)

**Summary**: A real-to-sim-to-real pipeline that mines everyday videos to produce whole-body control policies for context-aware humanoid behaviors like stair climbing and sitting.

---

### 12. Expert-Guided Imitation for Learning Humanoid Loco-Manipulation from Motion Capture
**Author**: Rohan P. Singh, Pierre-Alexandre Leziart, Masaki Murooka, Mitsuharu Morisawa, Eiichi Yoshida, Fumio Kanehiro

**Links**:
- arXiv link not available in search results

**Summary**: Expert-guided imitation learning approach for humanoid loco-manipulation using motion capture data.

---

### 13. Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching
**Authors**: Information not available in provided data

**Links**:
- Information not available

**Summary**: Research on autonomous delivery systems for humanoid robots integrating navigation, locomotion, and reaching capabilities.

---

## Notes
- Most papers focus on sim-to-real transfer for humanoid robots
- Common platforms: Unitree G1, H1
- Key techniques: Reinforcement Learning, Diffusion Models, Motion Retargeting, Vision-Language Models
- Datasets: AMASS, HumanML3D, LAFAN1, OMOMO

## Contributing
Feel free to submit pull requests to add more papers or update information.
