# Acrobatic & Body Motion

ê³ ë„ë¡œ ë™ì ì¸ ì „ì‹  ì›€ì§ì„ - Highly dynamic skills: kung-fu, balance, parkour

---

## GMT: General Motion Tracking for Humanoid Whole-Body Control

**Authors**: Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, Xiaolong Wang

**Year**: 2025

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2506.14770)
- ğŸŒ [Project Page](https://gmt-humanoid.github.io/)
- ğŸ’» [GitHub](https://github.com/zixuan417/humanoid-general-motion-tracking)

### ìš”ì•½ (Summary)

 GMT proposes a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training, while the MoE ensures better specialization of different regions of the motion manifold. GMT achieves state-of-the-art performance across a broad spectrum of motions using a unified general policy, including stretching, kicking, dancing, high kicking, kung-fu, boxing, running, side stepping, and squatting.

 GMTëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì´ ì‹¤ì œ ì„¸ê³„ì—ì„œ ë‹¤ì–‘í•œ ë™ì‘ì„ ì¶”ì í•  ìˆ˜ ìˆë„ë¡ ë‹¨ì¼ í†µí•© ì •ì±…ì„ í›ˆë ¨í•˜ëŠ” ì¼ë°˜ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ëª¨ì…˜ ì¶”ì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. GMTëŠ” ë‘ ê°€ì§€ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ ì ì‘í˜• ìƒ˜í”Œë§ ì „ëµê³¼ ëª¨ì…˜ Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì ì‘í˜• ìƒ˜í”Œë§ì€ í›ˆë ¨ ì¤‘ ì‰¬ìš´ ë™ì‘ê³¼ ì–´ë ¤ìš´ ë™ì‘ì˜ ê· í˜•ì„ ìë™ìœ¼ë¡œ ë§ì¶”ê³ , MoEëŠ” ëª¨ì…˜ ë§¤ë‹ˆí´ë“œì˜ ë‹¤ë¥¸ ì˜ì—­ì— ëŒ€í•œ ë” ë‚˜ì€ ì „ë¬¸í™”ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤. GMTëŠ” ìŠ¤íŠ¸ë ˆì¹­, í‚¥, ëŒ„ìŠ¤, í•˜ì´ í‚¥, ì¿µí‘¸, ë³µì‹±, ë‹¬ë¦¬ê¸°, ì¸¡ë©´ ìŠ¤í…, ìŠ¤ì¿¼íŠ¸ë¥¼ í¬í•¨í•œ ê´‘ë²”ìœ„í•œ ë™ì‘ ìŠ¤í™íŠ¸ëŸ¼ì— ê±¸ì³ í†µí•© ì¼ë°˜ ì •ì±…ì„ ì‚¬ìš©í•˜ì—¬ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.

### Key Features
- Unified general motion tracking policy
- Adaptive Sampling strategy for balanced training
- Motion Mixture-of-Experts (MoE) architecture
- State-of-the-art tracking performance
- Diverse motion capabilities (locomotion + acrobatics)
- Real-world deployment on Unitree G1
- Single policy for all motion types

---

## BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion

**Authors**: Qiayuan Liao, Takara E. Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, C. Karen Liu

**Year**: 2025

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2508.08241)
- ğŸŒ [Project Page](https://beyondmimic.github.io/)
- ğŸ’» [GitHub](https://github.com/HybridRobotics/whole_body_tracking)

### ìš”ì•½ (Summary)

 BeyondMimic presents a real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. The framework provides a motion tracking pipeline capable of challenging skills such as jumping spins, sprinting, and cartwheels with state-of-the-art motion quality. Moving beyond simply mimicking existing motions, BeyondMimic introduces a unified diffusion policy that enables zero-shot task-specific control at test time using simple cost functions. Deployed on hardware, BeyondMimic performs diverse tasks at test time, including waypoint navigation, joystick teleoperation, and obstacle avoidance, bridging sim-to-real motion tracking and flexible synthesis of human motion primitives for whole-body control.

 BeyondMimicì€ ê°€ì´ë“œ í™•ì‚°ì„ í†µí•œ ë‹¤ì¬ë‹¤ëŠ¥í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ íœ´ë¨¸ë…¸ì´ë“œ ì œì–´ë¥¼ ìœ„í•´ ì¸ê°„ ë™ì‘ìœ¼ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì‹¤ì œ ì„¸ê³„ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. í”„ë ˆì„ì›Œí¬ëŠ” ìµœì²¨ë‹¨ ëª¨ì…˜ í’ˆì§ˆë¡œ ì í”„ ìŠ¤í•€, ì „ë ¥ ì§ˆì£¼, ì¬ì£¼ë„˜ê¸°ì™€ ê°™ì€ ë„ì „ì ì¸ ê¸°ìˆ ì´ ê°€ëŠ¥í•œ ëª¨ì…˜ ì¶”ì  íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤. ê¸°ì¡´ ë™ì‘ì„ ë‹¨ìˆœíˆ ëª¨ë°©í•˜ëŠ” ê²ƒì„ ë„˜ì–´, BeyondMimicì€ ê°„ë‹¨í•œ ë¹„ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì‹œì ì— ì œë¡œìƒ· ì‘ì—…ë³„ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í†µí•© í™•ì‚° ì •ì±…ì„ ë„ì…í•©ë‹ˆë‹¤. í•˜ë“œì›¨ì–´ì— ë°°ì¹˜ëœ BeyondMimicì€ ê²½ìœ ì§€ ë„¤ë¹„ê²Œì´ì…˜, ì¡°ì´ìŠ¤í‹± ì›ê²©ì¡°ì‘, ì¥ì• ë¬¼ íšŒí”¼ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì‘ì—…ì„ í…ŒìŠ¤íŠ¸ ì‹œì ì— ìˆ˜í–‰í•˜ì—¬ ì „ì‹  ì œì–´ë¥¼ ìœ„í•œ ì‹¤ì œ-ì‹¤ì œ ëª¨ì…˜ ì¶”ì ê³¼ ì¸ê°„ ëª¨ì…˜ í”„ë¦¬ë¯¸í‹°ë¸Œì˜ ìœ ì—°í•œ í•©ì„±ì„ ì—°ê²°í•©ë‹ˆë‹¤.

### Key Features
- Scalable motion tracking pipeline
- Guided diffusion for policy synthesis
- State-of-the-art motion quality
- Zero-shot task adaptation at test time
- Challenging skills (jumping spins, sprinting, cartwheels)
- Real-world deployment on Unitree G1
- Unified diffusion policy for diverse tasks
- Cost function-based guidance

---

## HuB: Learning Extreme Humanoid Balance

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/pdf/2505.07294)
- ğŸŒ [Project](https://hub-robot.github.io/)
- ğŸ’» [GitHub] TBA

### ìš”ì•½ (Summary)

 HuB focuses on learning extreme humanoid balance capabilities. The system enables humanoids to maintain balance in challenging scenarios, recover from perturbations, and perform dynamic balancing tasks that push the limits of stability.

 HuBëŠ” ê·¹í•œì˜ íœ´ë¨¸ë…¸ì´ë“œ ê· í˜• ëŠ¥ë ¥ í•™ìŠµì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ íœ´ë¨¸ë…¸ì´ë“œê°€ ë„ì „ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ê· í˜•ì„ ìœ ì§€í•˜ê³ , ì„­ë™ìœ¼ë¡œë¶€í„° íšŒë³µí•˜ë©°, ì•ˆì •ì„±ì˜ í•œê³„ë¥¼ ì‹œí—˜í•˜ëŠ” ë™ì  ê· í˜• ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Extreme balance learning
- Perturbation recovery
- Dynamic balancing tasks
- Robust stability control
- Real-world deployment capabilities

---

## Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics Motions

**Authors**: Ziwen Zhuang, Hang Zhao

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/pdf/2502.01465)
- ğŸŒ [Project](https://project-instinct.github.io/)
- ğŸ’» [GitHub] TBA

### ìš”ì•½ (Summary)

 Embrace Collisions presents a humanoid shadowing approach for deployable contact-agnostic motions. Unlike traditional methods that avoid contact, this system learns to embrace and utilize collisions, enabling more natural and robust human-like movements including sitting, lying down, getting up from the ground, and transitioning between various postures.

 Embrace CollisionsëŠ” ë°°ì¹˜ ê°€ëŠ¥í•œ ì ‘ì´‰ ë¬´ê´€ ë™ì‘ì„ ìœ„í•œ íœ´ë¨¸ë…¸ì´ë“œ ì„€ë„ì‰ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì ‘ì´‰ì„ í”¼í•˜ëŠ” ì „í†µì ì¸ ë°©ë²•ê³¼ ë‹¬ë¦¬, ì´ ì‹œìŠ¤í…œì€ ì¶©ëŒì„ ìˆ˜ìš©í•˜ê³  í™œìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ì—¬ ì•‰ê¸°, ëˆ•ê¸°, ë°”ë‹¥ì—ì„œ ì¼ì–´ë‚˜ê¸°, ë‹¤ì–‘í•œ ìì„¸ ê°„ ì „í™˜ì„ í¬í•¨í•œ ë” ìì—°ìŠ¤ëŸ½ê³  ê²¬ê³ í•œ ì¸ê°„ê³¼ ê°™ì€ ì›€ì§ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Contact-agnostic motion learning
- Collision-aware control
- Natural whole-body locomotion
- Posture transition capabilities
- Recovery from ground positions
- Robust real-world deployment

---
## KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/pdf/2506.12851)
- ğŸŒ [Project](https://kungfu-bot.github.io/)
- ğŸ’» [GitHub](https://github.com/TeleHuman/PBHC) 

### ìš”ì•½ (Summary)

 KungfuBot presents a physics-based humanoid whole-body control system for learning highly-dynamic skills such as kung-fu movements. The system leverages reinforcement learning to master complex, coordinated motions that require precise timing and balance.

: KungfuBotì€ ì¿µí‘¸ ë™ì‘ê³¼ ê°™ì€ ê³ ë„ë¡œ ë™ì ì¸ ê¸°ìˆ ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë¬¼ë¦¬ ê¸°ë°˜ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì •ë°€í•œ íƒ€ì´ë°ê³¼ ê· í˜•ì´ í•„ìš”í•œ ë³µì¡í•˜ê³  í˜‘ì‘ëœ ë™ì‘ì„ ë§ˆìŠ¤í„°í•˜ê¸° ìœ„í•´ ê°•í™”í•™ìŠµì„ í™œìš©í•©ë‹ˆë‹¤.

### Key Features
- Physics-based control for dynamic skills
- Kung-fu and martial arts movements
- Highly coordinated whole-body motions
- RL-based skill learning
- Real-time balance control

---

## Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control

**Authors**: Xiaoyu Huang, Takara Truong, Yunbo Zhang, Fangzhou Yu, Jean-Pierre Sleiman, Jessica Hodgins, K. Sreenath, Farbod Farshidian

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2503.11801)
- ğŸŒ [Project](https://diffusecloc.github.io/website/)
- ğŸ’» [GitHub] TBA

### ìš”ì•½ (Summary)

 Diffuse-CLoC introduces a guided diffusion approach for physics-based character control. The method combines diffusion models with look-ahead control to generate realistic and physically plausible character motions.

 Diffuse-CLoCì€ ë¬¼ë¦¬ ê¸°ë°˜ ìºë¦­í„° ì œì–´ë¥¼ ìœ„í•œ ê°€ì´ë“œ í™•ì‚° ì ‘ê·¼ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ í™•ì‚° ëª¨ë¸ê³¼ ì„ í–‰ ì œì–´ë¥¼ ê²°í•©í•˜ì—¬ í˜„ì‹¤ì ì´ê³  ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œ ìºë¦­í„° ë™ì‘ì„ ìƒì„±í•©ë‹ˆë‹¤.

### Key Features
- Guided diffusion for character control
- Physics-based motion generation
- Look-ahead control mechanism
- Realistic character animation

---

## Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics

**Authors**: Tianyu Li, Jeonghwan Kim, Wontaek Kim, Donghoon Baek, Seungeun Rho, Sehoon Ha

**Links**:
- ğŸ“„ [arXiv](https://www.arxiv.org/pdf/2508.13444)
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

 Switch4EAI leverages console game platforms for benchmarking robotic athletics. The system uses game environments to train and evaluate athletic skills for humanoid robots, providing a scalable and diverse testing platform for dynamic movements.

 Switch4EAIëŠ” ë¡œë´‡ ìš´ë™ ëŠ¥ë ¥ ë²¤ì¹˜ë§ˆí‚¹ì„ ìœ„í•´ ì½˜ì†” ê²Œì„ í”Œë«í¼ì„ í™œìš©í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ê²Œì„ í™˜ê²½ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ìš´ë™ ê¸°ìˆ ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ë©°, ë™ì  ì›€ì§ì„ì„ ìœ„í•œ í™•ì¥ ê°€ëŠ¥í•˜ê³  ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ í”Œë«í¼ì„ ì œê³µí•©ë‹ˆë‹¤.

### Key Features
- Console game platform integration
- Athletic skill benchmarking
- Scalable training environment
- Diverse motion testing
- Game-based skill evaluation

---

## DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills

**Authors**: Xue Bin Peng, Pieter Abbeel, Sergey Levine, Michiel van de Panne

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/1804.02717)
- ğŸŒ [Project](https://xbpeng.github.io/projects/DeepMimic/index.html)
- ğŸ’» [GitHub](https://github.com/xbpeng/DeepMimic)

### ìš”ì•½ (Summary)

 DeepMimic presents a data-driven approach to physics-based character animation using deep reinforcement learning. The system learns to imitate reference motion clips through RL, enabling characters to perform a wide variety of challenging skills including locomotion, acrobatics, and martial arts.

 DeepMimicì€ ì‹¬ì¸µ ê°•í™”í•™ìŠµì„ ì‚¬ìš©í•œ ë¬¼ë¦¬ ê¸°ë°˜ ìºë¦­í„° ì• ë‹ˆë©”ì´ì…˜ì— ëŒ€í•œ ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ RLì„ í†µí•´ ì°¸ì¡° ëª¨ì…˜ í´ë¦½ì„ ëª¨ë°©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ì—¬ ìºë¦­í„°ê°€ ë³´í–‰, ê³¡ì˜ˆ, ë¬´ìˆ ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ë„ì „ì ì¸ ê¸°ìˆ ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Physics-based character control
- Example-guided reinforcement learning
- Diverse skill learning (locomotion, acrobatics, martial arts)
- Motion imitation framework
- Deep RL for character animation

---
[Back to Main README](../README.md)
