# Whole-Body Control + Object Interaction + Navigation

ê°ì²´ ìƒí˜¸ì‘ìš©, ì´ë™, ì œì–´ì˜ í†µí•© - Loco-manipulation with scene interaction

---

## OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: OmniRetarget presents a method for interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction. The system ensures that when human motion data is retargeted to humanoid robots, the interaction characteristics with objects and the environment are preserved, enabling more realistic and functional motion transfer.

**í•œê¸€**: OmniRetargetì€ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ë¡œì½”-ë§¤ë‹ˆí“°ë ˆì´ì…˜ ë° ì¥ë©´ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ìƒí˜¸ì‘ìš© ë³´ì¡´ ë°ì´í„° ìƒì„± ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì¸ê°„ ë™ì‘ ë°ì´í„°ê°€ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì— ë¦¬íƒ€ê²ŒíŒ…ë  ë•Œ ë¬¼ì²´ ë° í™˜ê²½ê³¼ì˜ ìƒí˜¸ì‘ìš© íŠ¹ì„±ì´ ë³´ì¡´ë˜ë„ë¡ í•˜ì—¬ ë” í˜„ì‹¤ì ì´ê³  ê¸°ëŠ¥ì ì¸ ëª¨ì…˜ ì „ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Interaction-preserving retargeting
- Whole-body loco-manipulation
- Scene interaction capabilities
- Contact-aware motion transfer
- Realistic human-to-humanoid mapping

---

## DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion

**Authors**: Dvij Kalaria, Sudarshan Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, S. Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Huang

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2509.14353)
- ğŸŒ [Project Page](https://genrobo.github.io/DreamControl/)
- ğŸ’» GitHub: TBA (Coming Soon)

### ìš”ì•½ (Summary)

**English**: DreamControl introduces a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): the core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). The system demonstrates that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural-looking motions, aiding in sim-to-real transfer. DreamControl validates its effectiveness on a Unitree G1 robot across diverse challenging tasks involving simultaneous lower and upper body control and object interaction, including opening drawers, bimanual picking, pressing buttons, and more.

**í•œê¸€**: DreamControlì€ ììœ¨ ì „ì‹  íœ´ë¨¸ë…¸ì´ë“œ ê¸°ìˆ  í•™ìŠµì„ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì†Œê°œí•©ë‹ˆë‹¤. DreamControlì€ í™•ì‚° ëª¨ë¸ê³¼ ê°•í™”í•™ìŠµ(RL)ì˜ ê°•ì ì„ í™œìš©í•©ë‹ˆë‹¤: í•µì‹¬ í˜ì‹ ì€ ì¸ê°„ ë™ì‘ ë°ì´í„°ë¡œ í›ˆë ¨ëœ í™•ì‚° ì‚¬ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë©°, ì´ëŠ” ì‹œë®¬ë ˆì´ì…˜ì—ì„œ RL ì •ì±…ì„ ì•ˆë‚´í•˜ì—¬ íŠ¹ì • ê´€ì‹¬ ì‘ì—…(ì˜ˆ: ì„œë ì—´ê¸° ë˜ëŠ” ë¬¼ì²´ ì§‘ê¸°)ì„ ì™„ë£Œí•˜ë„ë¡ í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì´ ì¸ê°„ ë™ì‘ ì •ë³´ê°€ ë‹´ê¸´ ì‚¬ì „ì´ RLì´ ì§ì ‘ RLë¡œëŠ” ë‹¬ì„±í•  ìˆ˜ ì—†ëŠ” ì†”ë£¨ì…˜ì„ ë°œê²¬í•  ìˆ˜ ìˆê²Œ í•˜ê³ , í™•ì‚° ëª¨ë¸ì´ ë³¸ì§ˆì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘ì„ ì´‰ì§„í•˜ì—¬ ì‹¤ì œ-ì‹¤ì œ ì „ì´ë¥¼ ë•ëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DreamControlì€ ì„œë ì—´ê¸°, ì–‘ì† ì§‘ê¸°, ë²„íŠ¼ ëˆ„ë¥´ê¸° ë“±ì„ í¬í•¨í•˜ì—¬ ë™ì‹œ í•˜ì²´ ë° ìƒì²´ ì œì–´ì™€ ë¬¼ì²´ ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•˜ëŠ” ë‹¤ì–‘í•œ ë„ì „ì ì¸ ì‘ì—…ì—ì„œ Unitree G1 ë¡œë´‡ì— ëŒ€í•œ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

### Key Features
- Diffusion-guided RL for humanoid control
- Human motion-informed priors
- Whole-body scene interaction
- Multiple task capabilities (drawer, picking, button pressing)
- Natural motion generation
- Sim-to-real transfer
- OmniControl diffusion model integration

---

## HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos

**Authors**: Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, Guanya Shi

**Links**:
- ğŸ“„ arXiv: arXiv:2509.16757v3
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: HDMI presents a framework for learning interactive humanoid whole-body control from human videos. The system leverages large-scale human video data to learn interactive behaviors, enabling humanoids to understand and replicate complex human-object and human-scene interactions observed in videos.

**í•œê¸€**: HDMIëŠ” ì¸ê°„ ë¹„ë””ì˜¤ë¡œë¶€í„° ìƒí˜¸ì‘ìš©ì  íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ë¥¼ í•™ìŠµí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ëŒ€ê·œëª¨ ì¸ê°„ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ìƒí˜¸ì‘ìš© í–‰ë™ì„ í•™ìŠµí•˜ê³ , íœ´ë¨¸ë…¸ì´ë“œê°€ ë¹„ë””ì˜¤ì—ì„œ ê´€ì°°ëœ ë³µì¡í•œ ì¸ê°„-ë¬¼ì²´ ë° ì¸ê°„-ì¥ë©´ ìƒí˜¸ì‘ìš©ì„ ì´í•´í•˜ê³  ì¬í˜„í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Learning from human videos
- Interactive whole-body control
- Scene interaction capabilities
- Video-based skill learning
- Human-object interaction understanding

---

[Back to Main README](../README.md)
