# Whole-Body Control + Object Interaction + Navigation

객체 상호작용, 이동, 제어의 통합 - Loco-manipulation with scene interaction

---

## OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction

**Links**:
- 📄 [arXiv](https://arxiv.org/pdf/2509.26633)
- 🌐 [Project](https://omniretarget.github.io/)
- 💻 GitHub: TBA (Coming Soon)
### 요약 (Summary)

 OmniRetarget presents a method for interaction-preserving data generation for humanoid whole-body loco-manipulation and scene interaction. The system ensures that when human motion data is retargeted to humanoid robots, the interaction characteristics with objects and the environment are preserved, enabling more realistic and functional motion transfer.

 OmniRetarget은 휴머노이드 전신 로코-매니퓰레이션 및 장면 상호작용을 위한 상호작용 보존 데이터 생성 방법을 제시합니다. 시스템은 인간 동작 데이터가 휴머노이드 로봇에 리타게팅될 때 물체 및 환경과의 상호작용 특성이 보존되도록 하여 더 현실적이고 기능적인 모션 전이를 가능하게 합니다.

### Key Features
- Interaction-preserving retargeting
- Whole-body loco-manipulation
- Scene interaction capabilities
- Contact-aware motion transfer
- Realistic human-to-humanoid mapping

---

## DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion

**Authors**: Dvij Kalaria, Sudarshan Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, S. Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Huang

**Links**:
- 📄 [arXiv](https://arxiv.org/abs/2509.14353)
- 🌐 [Project Page](https://genrobo.github.io/DreamControl/)
- 💻 GitHub: TBA (Coming Soon)

### 요약 (Summary)

 DreamControl introduces a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): the core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). The system demonstrates that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural-looking motions, aiding in sim-to-real transfer. DreamControl validates its effectiveness on a Unitree G1 robot across diverse challenging tasks involving simultaneous lower and upper body control and object interaction, including opening drawers, bimanual picking, pressing buttons, and more.

 DreamControl은 자율 전신 휴머노이드 기술 학습을 위한 새로운 방법론을 소개합니다. DreamControl은 확산 모델과 강화학습(RL)의 강점을 활용합니다: 핵심 혁신은 인간 동작 데이터로 훈련된 확산 사전을 사용하는 것이며, 이는 시뮬레이션에서 RL 정책을 안내하여 특정 관심 작업(예: 서랍 열기 또는 물체 집기)을 완료하도록 합니다. 시스템은 이 인간 동작 정보가 담긴 사전이 RL이 직접 RL로는 달성할 수 없는 솔루션을 발견할 수 있게 하고, 확산 모델이 본질적으로 자연스러운 동작을 촉진하여 실제-실제 전이를 돕는다는 것을 보여줍니다. DreamControl은 서랍 열기, 양손 집기, 버튼 누르기 등을 포함하여 동시 하체 및 상체 제어와 물체 상호작용을 포함하는 다양한 도전적인 작업에서 Unitree G1 로봇에 대한 효과를 검증합니다.

### Key Features
- Diffusion-guided RL for humanoid control
- Human motion-informed priors
- Whole-body scene interaction
- Multiple task capabilities (drawer, picking, button pressing)
- Natural motion generation
- Sim-to-real transfer
- OmniControl diffusion model integration

---

## HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos

**Authors**: Haoyang Weng, Yitang Li, Nikhil Sobanbabu, Zihan Wang, Zhengyi Luo, Tairan He, Deva Ramanan, Guanya Shi

**Links**:
- 📄 [arXiv](https://arxiv.org/abs/2509.16757)
- 🌐 [Project](https://hdmi-humanoid.github.io/#/)
- 💻 [GitHub](https://github.com/LeCAR-Lab/HDMI)

### 요약 (Summary)

 HDMI presents a framework for learning interactive humanoid whole-body control from human videos. The system leverages large-scale human video data to learn interactive behaviors, enabling humanoids to understand and replicate complex human-object and human-scene interactions observed in videos.

 HDMI는 인간 비디오로부터 상호작용적 휴머노이드 전신 제어를 학습하는 프레임워크를 제시합니다. 시스템은 대규모 인간 비디오 데이터를 활용하여 상호작용 행동을 학습하고, 휴머노이드가 비디오에서 관찰된 복잡한 인간-물체 및 인간-장면 상호작용을 이해하고 재현할 수 있게 합니다.

### Key Features
- Learning from human videos
- Interactive whole-body control
- Scene interaction capabilities
- Video-based skill learning
- Human-object interaction understanding

---

## Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space

**Authors**: Zhikai Zhang, Chao Chen, Han Xue, Jilong Wang, Sikai Liang, Yun Liu, Zongzhang Zhang, He Wang, Li Yi

**Year**: 2025

**Links**:
- 📄 [arXiv](https://arxiv.org/abs/2505.10918)
- 🌐 [Project](https://zzk273.github.io/R2S2/)
- 💻 [GitHub](https://github.com/GalaxyGeneralRobotics/OpenWBT)

### 요약 (Summary)

 This work presents a framework for unleashing humanoid reaching potential through a real-world-ready skill space. The system enables humanoids to perform diverse reaching and manipulation tasks with high precision and adaptability. By learning in a structured skill space, the humanoid can generalize to various reaching scenarios and object interactions.

 이 연구는 실제 세계 준비가 된 기술 공간을 통해 휴머노이드의 도달 잠재력을 발휘하는 프레임워크를 제시합니다. 시스템은 휴머노이드가 높은 정밀도와 적응성으로 다양한 도달 및 조작 작업을 수행할 수 있게 합니다. 구조화된 기술 공간에서 학습함으로써 휴머노이드는 다양한 도달 시나리오와 물체 상호작용에 일반화할 수 있습니다.

### Key Features
- Real-world-ready skill space
- Diverse reaching capabilities
- High-precision manipulation
- Adaptive task execution
- Structured skill learning
- Generalization to various scenarios

---

## AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control

**Authors**: Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, Xiaolong Wang

**Links**:
- 📄 [arXiv](https://arxiv.org/abs/2505.03738)
- 🌐 [Project](https://amo-humanoid.github.io/)
- 💻 [GitHub](https://github.com/OpenTeleVision/AMO)

### 요약 (Summary)

 AMO introduces adaptive motion optimization for hyper-dexterous humanoid whole-body control. The system optimizes motion in real-time to achieve highly dexterous manipulation while maintaining whole-body balance and coordination.

 AMO는 초정밀 휴머노이드 전신 제어를 위한 적응형 모션 최적화를 소개합니다. 시스템은 전신 균형과 협응을 유지하면서 매우 정밀한 조작을 달성하기 위해 실시간으로 모션을 최적화합니다.

### Key Features
- Real-time motion optimization
- Hyper-dexterous manipulation capabilities
- Adaptive whole-body control
- Balance maintenance during manipulation

---

## HumanPlus: Humanoid Shadowing and Imitation from Humans

**Authors**: Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn

**Conference**: CoRL 2024

**Links**:
- 📄 [arXiv](https://arxiv.org/abs/2406.10454)
- 🌐 [Project](https://humanoid-ai.github.io/)
- 💻 [GitHub](https://github.com/MarkFzp/humanplus)

### 요약 (Summary)

 HumanPlus introduces a full-stack system for humanoids to learn motion and autonomous skills from human data. It first trains a low-level policy in simulation via RL using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only an RGB camera (shadowing). Through shadowing, operators can teleoperate humanoids to collect whole-body data for learning different tasks. The system demonstrates autonomous completion of tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations.

 HumanPlus는 휴머노이드가 인간 데이터로부터 동작 및 자율 기술을 학습하는 풀스택 시스템을 소개합니다. 먼저 기존 40시간 분량의 인간 동작 데이터셋을 사용하여 시뮬레이션에서 RL을 통해 저수준 정책을 훈련합니다. 이 정책은 실제 세계로 전이되어 휴머노이드 로봇이 RGB 카메라만을 사용하여 실시간으로 인간의 신체와 손 동작을 따라할 수 있게 합니다(섀도잉). 섀도잉을 통해 운영자는 휴머노이드를 원격조작하여 다양한 작업 학습을 위한 전신 데이터를 수집할 수 있습니다. 시스템은 신발 신고 일어나 걷기, 창고 랙에서 물건 내리기, 운동복 접기, 물건 재배치, 타이핑, 다른 로봇과 인사하기 등의 작업을 최대 40개의 시연을 사용하여 60-100% 성공률로 자율 완료합니다.

### Key Features
- Humanoid Shadowing Transformer (HST) for low-level control
- Humanoid Imitation Transformer (HIT) for skill learning
- 40+ hours of human motion data training
- RGB camera-only shadowing system
- 33-DoF 180cm humanoid platform
- Real-world task completion (60-100% success rates)

---


[Back to Main README](../README.md)
