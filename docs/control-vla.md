# Whole-Body Control + VLA (Vision-Language-Action)

비전-언어-행동 모델 기반 제어 - Vision-Language-Action models for humanoid control

---

## LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning

**Authors**: Yiyang Shao, Bike Zhang, Qiayuan Liao, Xiaoyu Huang, Yuman Gao, Yufeng Chi, Zhongyu Li, Sophia Shao, Koushil Sreenath

**Links**:
- 📄 arXiv: TBA
- 🌐 Project Page: TBA

### 요약 (Summary)

**English**: LangWBC presents a language-directed humanoid whole-body control system via end-to-end learning. The system enables users to control humanoids using natural language commands, translating high-level linguistic instructions into coordinated whole-body motions. This bridges the gap between human intent expressed through language and robot execution.

**한글**: LangWBC는 엔드투엔드 학습을 통한 언어 지향 휴머노이드 전신 제어 시스템을 제시합니다. 시스템은 사용자가 자연어 명령을 사용하여 휴머노이드를 제어할 수 있게 하며, 고수준 언어 지시를 협응된 전신 동작으로 변환합니다. 이는 언어를 통해 표현된 인간의 의도와 로봇 실행 사이의 격차를 연결합니다.

### Key Features
- Natural language control interface
- End-to-end learning framework
- High-level command interpretation
- Whole-body motion generation from language
- Language-to-action translation

---

## LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction

**Links**:
- 📄 arXiv: TBA
- 🌐 Project Page: TBA

### 요약 (Summary)

**English**: LeVERB introduces humanoid whole-body control with latent vision-language instruction. The system uses latent representations learned from vision and language to guide humanoid control, enabling more robust and generalizable behavior across diverse tasks and environments.

**한글**: LeVERB는 잠재 비전-언어 지시를 사용한 휴머노이드 전신 제어를 소개합니다. 시스템은 비전과 언어로부터 학습한 잠재 표현을 사용하여 휴머노이드 제어를 안내하며, 다양한 작업과 환경에서 더 견고하고 일반화 가능한 행동을 가능하게 합니다.

### Key Features
- Latent vision-language representations
- Multimodal instruction following
- Whole-body control integration
- Robust task generalization
- Vision-language alignment

---

## Visual Imitation Enables Contextual Humanoid Control

**Links**:
- 📄 arXiv: TBA
- 🌐 Project Page: TBA

### 요약 (Summary)

**English**: This work presents visual imitation for contextual humanoid control. The system learns to control humanoids by imitating behaviors observed in visual data, enabling context-aware control that adapts to environmental and task-specific conditions.

**한글**: 이 연구는 맥락적 휴머노이드 제어를 위한 시각적 모방을 제시합니다. 시스템은 시각 데이터에서 관찰된 행동을 모방하여 휴머노이드를 제어하는 방법을 학습하며, 환경 및 작업별 조건에 적응하는 맥락 인식 제어를 가능하게 합니다.

### Key Features
- Visual imitation learning
- Context-aware control
- Vision-based behavior learning
- Environmental adaptation
- Task-specific policy learning

---

[Back to Main README](../README.md)
