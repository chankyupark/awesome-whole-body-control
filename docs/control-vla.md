# Whole-Body Control + VLA (Vision-Language-Action)

ë¹„ì „-ì–¸ì–´-í–‰ë™ ëª¨ë¸ ê¸°ë°˜ ì œì–´ - Vision-Language-Action models for humanoid control

---

## LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning

**Authors**: Yiyang Shao, Bike Zhang, Qiayuan Liao, Xiaoyu Huang, Yuman Gao, Yufeng Chi, Zhongyu Li, Sophia Shao, Koushil Sreenath

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: LangWBC presents a language-directed humanoid whole-body control system via end-to-end learning. The system enables users to control humanoids using natural language commands, translating high-level linguistic instructions into coordinated whole-body motions. This bridges the gap between human intent expressed through language and robot execution.

**í•œê¸€**: LangWBCëŠ” ì—”ë“œíˆ¬ì—”ë“œ í•™ìŠµì„ í†µí•œ ì–¸ì–´ ì§€í–¥ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ìì—°ì–´ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œë¥¼ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ê³ ìˆ˜ì¤€ ì–¸ì–´ ì§€ì‹œë¥¼ í˜‘ì‘ëœ ì „ì‹  ë™ì‘ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ì–¸ì–´ë¥¼ í†µí•´ í‘œí˜„ëœ ì¸ê°„ì˜ ì˜ë„ì™€ ë¡œë´‡ ì‹¤í–‰ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

### Key Features
- Natural language control interface
- End-to-end learning framework
- High-level command interpretation
- Whole-body motion generation from language
- Language-to-action translation

---

## LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: LeVERB introduces humanoid whole-body control with latent vision-language instruction. The system uses latent representations learned from vision and language to guide humanoid control, enabling more robust and generalizable behavior across diverse tasks and environments.

**í•œê¸€**: LeVERBëŠ” ì ì¬ ë¹„ì „-ì–¸ì–´ ì§€ì‹œë¥¼ ì‚¬ìš©í•œ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ë¹„ì „ê³¼ ì–¸ì–´ë¡œë¶€í„° í•™ìŠµí•œ ì ì¬ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œ ì œì–´ë¥¼ ì•ˆë‚´í•˜ë©°, ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ì—ì„œ ë” ê²¬ê³ í•˜ê³  ì¼ë°˜í™” ê°€ëŠ¥í•œ í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Latent vision-language representations
- Multimodal instruction following
- Whole-body control integration
- Robust task generalization
- Vision-language alignment

---

## Visual Imitation Enables Contextual Humanoid Control

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: This work presents visual imitation for contextual humanoid control. The system learns to control humanoids by imitating behaviors observed in visual data, enabling context-aware control that adapts to environmental and task-specific conditions.

**í•œê¸€**: ì´ ì—°êµ¬ëŠ” ë§¥ë½ì  íœ´ë¨¸ë…¸ì´ë“œ ì œì–´ë¥¼ ìœ„í•œ ì‹œê°ì  ëª¨ë°©ì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì‹œê° ë°ì´í„°ì—ì„œ ê´€ì°°ëœ í–‰ë™ì„ ëª¨ë°©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œë¥¼ ì œì–´í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ë©°, í™˜ê²½ ë° ì‘ì—…ë³„ ì¡°ê±´ì— ì ì‘í•˜ëŠ” ë§¥ë½ ì¸ì‹ ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Visual imitation learning
- Context-aware control
- Vision-based behavior learning
- Environmental adaptation
- Task-specific policy learning

---

[Back to Main README](../README.md)
