# Whole-Body Control + VLA (Vision-Language-Action)

ë¹„ì „-ì–¸ì–´-í–‰ë™ ëª¨ë¸ ê¸°ë°˜ ì œì–´ - Vision-Language-Action models for humanoid control

---

## DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion

**Authors**: Dvij Kalaria, Sudarshan Harithas, Pushkal Katara, Sangkyung Kwak, Sarthak Bhagat, S. Shankar Sastry, Srinath Sridhar, Sai Vemprala, Ashish Kapoor, Jonathan Huang

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2509.14353)
- ğŸŒ [Project Page](https://genrobo.github.io/DreamControl/)
- ğŸ’» GitHub: TBA (Coming Soon)

### ìš”ì•½ (Summary)

 DreamControl introduces a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): the core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). The system demonstrates that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural-looking motions, aiding in sim-to-real transfer. DreamControl validates its effectiveness on a Unitree G1 robot across diverse challenging tasks involving simultaneous lower and upper body control and object interaction, including opening drawers, bimanual picking, pressing buttons, and more.

 DreamControlì€ ììœ¨ ì „ì‹  íœ´ë¨¸ë…¸ì´ë“œ ê¸°ìˆ  í•™ìŠµì„ ìœ„í•œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì†Œê°œí•©ë‹ˆë‹¤. DreamControlì€ í™•ì‚° ëª¨ë¸ê³¼ ê°•í™”í•™ìŠµ(RL)ì˜ ê°•ì ì„ í™œìš©í•©ë‹ˆë‹¤: í•µì‹¬ í˜ì‹ ì€ ì¸ê°„ ë™ì‘ ë°ì´í„°ë¡œ í›ˆë ¨ëœ í™•ì‚° ì‚¬ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë©°, ì´ëŠ” ì‹œë®¬ë ˆì´ì…˜ì—ì„œ RL ì •ì±…ì„ ì•ˆë‚´í•˜ì—¬ íŠ¹ì • ê´€ì‹¬ ì‘ì—…(ì˜ˆ: ì„œë ì—´ê¸° ë˜ëŠ” ë¬¼ì²´ ì§‘ê¸°)ì„ ì™„ë£Œí•˜ë„ë¡ í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì´ ì¸ê°„ ë™ì‘ ì •ë³´ê°€ ë‹´ê¸´ ì‚¬ì „ì´ RLì´ ì§ì ‘ RLë¡œëŠ” ë‹¬ì„±í•  ìˆ˜ ì—†ëŠ” ì†”ë£¨ì…˜ì„ ë°œê²¬í•  ìˆ˜ ìˆê²Œ í•˜ê³ , í™•ì‚° ëª¨ë¸ì´ ë³¸ì§ˆì ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘ì„ ì´‰ì§„í•˜ì—¬ ì‹¤ì œ-ì‹¤ì œ ì „ì´ë¥¼ ë•ëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. DreamControlì€ ì„œë ì—´ê¸°, ì–‘ì† ì§‘ê¸°, ë²„íŠ¼ ëˆ„ë¥´ê¸° ë“±ì„ í¬í•¨í•˜ì—¬ ë™ì‹œ í•˜ì²´ ë° ìƒì²´ ì œì–´ì™€ ë¬¼ì²´ ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•˜ëŠ” ë‹¤ì–‘í•œ ë„ì „ì ì¸ ì‘ì—…ì—ì„œ Unitree G1 ë¡œë´‡ì— ëŒ€í•œ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.

### Key Features
- Diffusion-guided RL for humanoid control
- Human motion-informed priors
- Whole-body scene interaction
- Multiple task capabilities (drawer, picking, button pressing)
- Natural motion generation
- Sim-to-real transfer
- OmniControl diffusion model integration

---

## LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2506.13751)
- ğŸŒ [Project](https://ember-lab-berkeley.github.io/LeVERB-Website/)
- ğŸ’» GitHub: TBA (Coming Soon)
### ìš”ì•½ (Summary)

 LeVERB introduces humanoid whole-body control with latent vision-language instruction. The system uses latent representations learned from vision and language to guide humanoid control, enabling more robust and generalizable behavior across diverse tasks and environments.

 LeVERBëŠ” ì ì¬ ë¹„ì „-ì–¸ì–´ ì§€ì‹œë¥¼ ì‚¬ìš©í•œ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ë¹„ì „ê³¼ ì–¸ì–´ë¡œë¶€í„° í•™ìŠµí•œ ì ì¬ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œ ì œì–´ë¥¼ ì•ˆë‚´í•˜ë©°, ë‹¤ì–‘í•œ ì‘ì—…ê³¼ í™˜ê²½ì—ì„œ ë” ê²¬ê³ í•˜ê³  ì¼ë°˜í™” ê°€ëŠ¥í•œ í–‰ë™ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Latent vision-language representations
- Multimodal instruction following
- Whole-body control integration
- Robust task generalization
- Vision-language alignment

---

## LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning

**Authors**: Yiyang Shao, Bike Zhang, Qiayuan Liao, Xiaoyu Huang, Yuman Gao, Yufeng Chi, Zhongyu Li, Sophia Shao, Koushil Sreenath

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2504.21738)
- ğŸŒ [Project](https://langwbc.github.io/)
- ğŸ’» [GitHub](https://github.com/YiyangShao2003/LangWBC)

### ìš”ì•½ (Summary)

 LangWBC presents a language-directed humanoid whole-body control system via end-to-end learning. The system enables users to control humanoids using natural language commands, translating high-level linguistic instructions into coordinated whole-body motions. This bridges the gap between human intent expressed through language and robot execution.

 LangWBCëŠ” ì—”ë“œíˆ¬ì—”ë“œ í•™ìŠµì„ í†µí•œ ì–¸ì–´ ì§€í–¥ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ìì—°ì–´ ëª…ë ¹ì„ ì‚¬ìš©í•˜ì—¬ íœ´ë¨¸ë…¸ì´ë“œë¥¼ ì œì–´í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ê³ ìˆ˜ì¤€ ì–¸ì–´ ì§€ì‹œë¥¼ í˜‘ì‘ëœ ì „ì‹  ë™ì‘ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ì–¸ì–´ë¥¼ í†µí•´ í‘œí˜„ëœ ì¸ê°„ì˜ ì˜ë„ì™€ ë¡œë´‡ ì‹¤í–‰ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.

### Key Features
- Natural language control interface
- End-to-end learning framework
- High-level command interpretation
- Whole-body motion generation from language
- Language-to-action translation

---




[Back to Main README](../README.md)
