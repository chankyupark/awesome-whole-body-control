# Whole-Body Oriented Teleoperation

ì „ì‹  í˜‘ì‘ ì›ê²©ì¡°ì‘ ì‹œìŠ¤í…œ - Coordinated full-body control systems

---

## CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks

**Authors**: Yixuan Li, Yutang Lin, Jieming Cui, Tengyu Liu, Wei Liang, Yixin Zhu, Siyuan Huang

**Year**: 2025

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2506.08931)
- ğŸŒ [Project Page](https://humanoid-clone.github.io/)
- ğŸ’» [GitHub](https://github.com/humanoid-clone/CLONE/)

### ìš”ì•½ (Summary)

**English**: CLONE is a closed-loop whole-body teleoperation system that achieves comprehensive robot control using only a VR headset. It employs a Mixture-of-Experts (MoE) architecture that learns diverse motion skills while a LiDAR-based error correction mechanism prevents the accumulation of positional drift. CLONE enables unprecedented whole-body teleoperation fidelity, maintaining minimal positional drift (5.1cm mean error over 8.9m) over long-range trajectories using only head and hand tracking from an MR headset.

**í•œê¸€**: CLONEì€ VR í—¤ë“œì…‹ë§Œì„ ì‚¬ìš©í•˜ì—¬ í¬ê´„ì ì¸ ë¡œë´‡ ì œì–´ë¥¼ ë‹¬ì„±í•˜ëŠ” íë£¨í”„ ì „ì‹  ì›ê²©ì¡°ì‘ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ì…˜ ê¸°ìˆ ì„ í•™ìŠµí•˜ëŠ” Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ë¥¼ ì±„ìš©í•˜ë©°, LiDAR ê¸°ë°˜ ì˜¤ë¥˜ ë³´ì • ë©”ì»¤ë‹ˆì¦˜ì´ ìœ„ì¹˜ ë“œë¦¬í”„íŠ¸ ì¶•ì ì„ ë°©ì§€í•©ë‹ˆë‹¤. CLONEì€ MR í—¤ë“œì…‹ì˜ ë¨¸ë¦¬ì™€ ì† ì¶”ì ë§Œìœ¼ë¡œ ì¥ê±°ë¦¬ ê¶¤ì ì—ì„œ ìµœì†Œí•œì˜ ìœ„ì¹˜ ë“œë¦¬í”„íŠ¸(8.9mì—ì„œ 5.1cm í‰ê·  ì˜¤ë¥˜)ë¥¼ ìœ ì§€í•˜ë©° ì „ë¡€ ì—†ëŠ” ì „ì‹  ì›ê²©ì¡°ì‘ ì •í™•ë„ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- MoE-based teleoperation framework
- Closed-loop error correction with LiDAR odometry
- Minimal hardware requirements (only MR headset)
- Long-horizon task capabilities (15m+ trajectories)
- Real-time coordination of upper and lower body
- CLONED dataset with augmented motion data

---

## HumanPlus: Humanoid Shadowing and Imitation from Humans

**Authors**: Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn

**Conference**: CoRL 2024

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2406.10454)
- ğŸŒ [Project Page](https://humanoid-ai.github.io/)
- ğŸ’» GitHub: TBA

### ìš”ì•½ (Summary)

**English**: HumanPlus introduces a full-stack system for humanoids to learn motion and autonomous skills from human data. It first trains a low-level policy in simulation via RL using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only an RGB camera (shadowing). Through shadowing, operators can teleoperate humanoids to collect whole-body data for learning different tasks. The system demonstrates autonomous completion of tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations.

**í•œê¸€**: HumanPlusëŠ” íœ´ë¨¸ë…¸ì´ë“œê°€ ì¸ê°„ ë°ì´í„°ë¡œë¶€í„° ë™ì‘ ë° ììœ¨ ê¸°ìˆ ì„ í•™ìŠµí•˜ëŠ” í’€ìŠ¤íƒ ì‹œìŠ¤í…œì„ ì†Œê°œí•©ë‹ˆë‹¤. ë¨¼ì € ê¸°ì¡´ 40ì‹œê°„ ë¶„ëŸ‰ì˜ ì¸ê°„ ë™ì‘ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ RLì„ í†µí•´ ì €ìˆ˜ì¤€ ì •ì±…ì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì´ ì •ì±…ì€ ì‹¤ì œ ì„¸ê³„ë¡œ ì „ì´ë˜ì–´ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì´ RGB ì¹´ë©”ë¼ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸ê°„ì˜ ì‹ ì²´ì™€ ì† ë™ì‘ì„ ë”°ë¼í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤(ì„€ë„ì‰). ì„€ë„ì‰ì„ í†µí•´ ìš´ì˜ìëŠ” íœ´ë¨¸ë…¸ì´ë“œë¥¼ ì›ê²©ì¡°ì‘í•˜ì—¬ ë‹¤ì–‘í•œ ì‘ì—… í•™ìŠµì„ ìœ„í•œ ì „ì‹  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì‹ ë°œ ì‹ ê³  ì¼ì–´ë‚˜ ê±·ê¸°, ì°½ê³  ë™ì—ì„œ ë¬¼ê±´ ë‚´ë¦¬ê¸°, ìš´ë™ë³µ ì ‘ê¸°, ë¬¼ê±´ ì¬ë°°ì¹˜, íƒ€ì´í•‘, ë‹¤ë¥¸ ë¡œë´‡ê³¼ ì¸ì‚¬í•˜ê¸° ë“±ì˜ ì‘ì—…ì„ ìµœëŒ€ 40ê°œì˜ ì‹œì—°ì„ ì‚¬ìš©í•˜ì—¬ 60-100% ì„±ê³µë¥ ë¡œ ììœ¨ ì™„ë£Œí•©ë‹ˆë‹¤.

### Key Features
- Humanoid Shadowing Transformer (HST) for low-level control
- Humanoid Imitation Transformer (HIT) for skill learning
- 40+ hours of human motion data training
- RGB camera-only shadowing system
- 33-DoF 180cm humanoid platform
- Real-world task completion (60-100% success rates)

---

## OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning

**Authors**: Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, Guanya Shi

**Conference**: CoRL 2024

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2406.08858)
- ğŸŒ [Project Page](https://omni.human2humanoid.com/)
- ğŸ’» [GitHub](https://github.com/LeCAR-Lab/human2humanoid)

### ìš”ì•½ (Summary)

**English**: OmniH2O presents a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including real-time teleoperation through VR headset, verbal instruction, and RGB camera. It also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. The system demonstrates versatility and dexterity in various real-world whole-body tasks such as playing multiple sports, moving and manipulating objects, and interacting with humans.

**í•œê¸€**: OmniH2OëŠ” ì „ì‹  íœ´ë¨¸ë…¸ì´ë“œ ì›ê²©ì¡°ì‘ ë° ììœ¨ì„±ì„ ìœ„í•œ í•™ìŠµ ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ìš´ë™í•™ì  í¬ì¦ˆë¥¼ ë²”ìš© ì œì–´ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬, OmniH2OëŠ” VR í—¤ë“œì…‹, ìŒì„± ëª…ë ¹, RGB ì¹´ë©”ë¼ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì›ê²©ì¡°ì‘ì„ í¬í•¨í•˜ì—¬ ì¸ê°„ì´ ì •ë°€í•œ ì†ì„ ê°€ì§„ ì „ì‹  í¬ê¸° íœ´ë¨¸ë…¸ì´ë“œë¥¼ ì œì–´í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ ì›ê²©ì¡°ì‘ ì‹œì—°ìœ¼ë¡œë¶€í„° í•™ìŠµí•˜ê±°ë‚˜ GPT-4ì™€ ê°™ì€ ìµœì²¨ë‹¨ ëª¨ë¸ê³¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ììœ¨ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ ìŠ¤í¬ì¸  ê²½ê¸°, ë¬¼ì²´ ì´ë™ ë° ì¡°ì‘, ì¸ê°„ê³¼ì˜ ìƒí˜¸ì‘ìš© ë“± ë‹¤ì–‘í•œ ì‹¤ì œ ì „ì‹  ì‘ì—…ì—ì„œ ë‹¤ì¬ë‹¤ëŠ¥í•¨ê³¼ ì •ë°€í•¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### Key Features
- Universal kinematic pose control interface
- Multiple teleoperation modalities (VR, voice, camera)
- GPT-4 integration for autonomy
- Dexterous hand control
- OmniH2O-6 dataset (6 everyday tasks)
- Robust sim-to-real transfer pipeline

---

## Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation

**Authors**: Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi

**Conference**: IROS 2024

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2403.04436)
- ğŸŒ [Project Page](https://human2humanoid.com/)
- ğŸ’» [GitHub](https://github.com/LeCAR-Lab/human2humanoid)

### ìš”ì•½ (Summary)

**English**: H2O presents a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset, the paper proposes a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. The trained policy successfully achieves teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, and boxing. This is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.

**í•œê¸€**: H2OëŠ” RGB ì¹´ë©”ë¼ë§Œìœ¼ë¡œ ì „ì‹  í¬ê¸° íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì˜ ì‹¤ì‹œê°„ ì „ì‹  ì›ê²©ì¡°ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ê°•í™”í•™ìŠµ(RL) ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ë¦¬íƒ€ê²ŒíŒ…ëœ ëª¨ì…˜ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê¸° ìœ„í•´, ë…¼ë¬¸ì€ íŠ¹ê¶Œì  ëª¨ì…˜ ëª¨ë°©ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ëª¨ì…˜ì„ í•„í„°ë§í•˜ê³  ì„ íƒí•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•œ "ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ë°ì´í„°ë¡œ" í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. í›ˆë ¨ëœ ì •ì±…ì€ ê±·ê¸°, ë’¤ë¡œ ì í”„í•˜ê¸°, ì°¨ê¸°, ëŒê¸°, ì† í”ë“¤ê¸°, ë°€ê¸°, ë³µì‹±ì„ í¬í•¨í•œ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë™ì  ì „ì‹  ë™ì‘ì˜ ì›ê²©ì¡°ì‘ì„ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ê²ƒì€ í•™ìŠµ ê¸°ë°˜ ì‹¤ì‹œê°„ ì „ì‹  íœ´ë¨¸ë…¸ì´ë“œ ì›ê²©ì¡°ì‘ì„ ë‹¬ì„±í•œ ì²« ë²ˆì§¸ ì‹œì—°ì…ë‹ˆë‹¤.

### Key Features
- First real-time whole-body teleoperation via learning
- RGB camera-only system
- "Sim-to-data" motion filtering process
- Dynamic motion capabilities (jumping, kicking, boxing)
- Zero-shot sim-to-real transfer
- AMASS dataset utilization

---

## TWIST: Teleoperated Whole-Body Imitation System

**Authors**: Yanjie Ze, Zixuan Chen, JoÃ£o Pedro AraÃºjo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, C. Karen Liu

**Year**: 2025

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2505.02833)
- ğŸŒ [Project Page](https://yanjieze.com/TWIST/)
- ğŸ’» [GitHub](https://github.com/YanjieZe/TWIST)

### ìš”ì•½ (Summary)

**English**: TWIST presents a system for humanoid teleoperation through whole-body motion imitation. The system generates reference motion clips by retargeting human motion capture data to the humanoid robot, then develops a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, TWIST demonstrates how incorporating privileged future motion frames and real-world motion capture data improves tracking accuracy. The system enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement using a single unified neural network controller.

**í•œê¸€**: TWISTëŠ” ì „ì‹  ë™ì‘ ëª¨ë°©ì„ í†µí•œ íœ´ë¨¸ë…¸ì´ë“œ ì›ê²©ì¡°ì‘ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì¸ê°„ ëª¨ì…˜ ìº¡ì²˜ ë°ì´í„°ë¥¼ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì— ë¦¬íƒ€ê²ŒíŒ…í•˜ì—¬ ì°¸ì¡° ëª¨ì…˜ í´ë¦½ì„ ìƒì„±í•œ ë‹¤ìŒ, ê°•í™”í•™ìŠµê³¼ í–‰ë™ ë³µì œ(RL+BC)ì˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ê²¬ê³ í•˜ê³  ì ì‘ì ì´ë©° ë°˜ì‘ì„± ìˆëŠ” ì „ì‹  ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ê°œë°œí•©ë‹ˆë‹¤. ì²´ê³„ì ì¸ ë¶„ì„ì„ í†µí•´ TWISTëŠ” íŠ¹ê¶Œì  ë¯¸ë˜ ëª¨ì…˜ í”„ë ˆì„ê³¼ ì‹¤ì œ ëª¨ì…˜ ìº¡ì²˜ ë°ì´í„°ë¥¼ í†µí•©í•˜ëŠ” ê²ƒì´ ì¶”ì  ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ë‹¨ì¼ í†µí•© ì‹ ê²½ë§ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì‹  ì¡°ì‘, ë‹¤ë¦¬ ì¡°ì‘, ì´ë™, í‘œí˜„ì  ì›€ì§ì„ì„ í¬ê´„í•˜ëŠ” ì „ë¡€ ì—†ì´ ë‹¤ì¬ë‹¤ëŠ¥í•˜ê³  í˜‘ì‘ëœ ì „ì‹  ìš´ë™ ê¸°ìˆ ì„ ì‹¤ì œ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì´ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### Key Features
- RL+BC hybrid training approach
- Whole-body motion retargeting from human MoCap
- Privileged future motion frame incorporation
- Versatile motor skills (manipulation, locomotion, expressive movement)
- Single unified neural network controller
- Real-time teleoperation capabilities

---

## Mobile-TeleVision: Predictive Motion Priors for Humanoid Whole-Body Control

**Authors**: Chenhao Lu, Xuxin Cheng, Jialong Li, Shiqi Yang, Mazeyu Ji, Chengjing Yuan, Ge Yang, Sha Yi, Xiaolong Wang

**Conference**: ICRA 2025

**Links**:
- ğŸ“„ [arXiv](https://arxiv.org/abs/2412.07773)
- ğŸŒ [Project Page](https://mobile-tv.github.io/)
- ğŸ’» GitHub: TBA

### ìš”ì•½ (Summary)

**English**: Mobile-TeleVision proposes decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. The key innovation is PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. The system significantly outperforms RL-based whole-body control in precise manipulation tasks.

**í•œê¸€**: Mobile-TeleVisionì€ ìƒì²´ ì œì–´ë¥¼ ì´ë™ìœ¼ë¡œë¶€í„° ë¶„ë¦¬í•˜ì—¬, ì •ë°€ ì¡°ì‘ì„ ìœ„í•´ ì—­ìš´ë™í•™(IK)ê³¼ ëª¨ì…˜ ë¦¬íƒ€ê²ŒíŒ…ì„ ì‚¬ìš©í•˜ê³ , RLì€ ê²¬ê³ í•œ í•˜ì²´ ì´ë™ì— ì§‘ì¤‘í•˜ë„ë¡ ì œì•ˆí•©ë‹ˆë‹¤. í•µì‹¬ í˜ì‹ ì€ ì¡°ê±´ë¶€ ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(CVAE)ë¡œ í›ˆë ¨ë˜ì–´ ìƒì²´ ë™ì‘ì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” PMP(ì˜ˆì¸¡ ëª¨ì…˜ ì‚¬ì „)ì…ë‹ˆë‹¤. ì´ë™ ì •ì±…ì€ ì´ ìƒì²´ ë™ì‘ í‘œí˜„ì— ì¡°ê±´í™”ë˜ì–´ í›ˆë ¨ë˜ë¯€ë¡œ, ì‹œìŠ¤í…œì´ ì¡°ì‘ê³¼ ì´ë™ ëª¨ë‘ì—ì„œ ê²¬ê³ í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì •ë°€ ì¡°ì‘ ì‘ì—…ì—ì„œ RL ê¸°ë°˜ ì „ì‹  ì œì–´ë¥¼ í¬ê²Œ ëŠ¥ê°€í•©ë‹ˆë‹¤.

### Key Features
- Decoupled upper-body and lower-body control
- Predictive Motion Priors (PMP) with CVAE
- IK-based precise upper-body manipulation
- RL-based robust lower-body locomotion
- Superior manipulation precision vs. end-to-end RL
- Real-world deployment on Fourier GR1 and Unitree H1

---

## HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit

**Authors**: Qingwei Ben, Feiyu Jia, Jia Zeng, Junting Dong, Dahua Lin, Jiangmiao Pang

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: HOMIE presents a humanoid loco-manipulation system using an isomorphic exoskeleton cockpit for teleoperation. The system enables intuitive whole-body control through a human-worn exoskeleton that mirrors the robot's morphology.

**í•œê¸€**: HOMIEëŠ” ì›ê²©ì¡°ì‘ì„ ìœ„í•œ ë™í˜• ì—‘ì†ŒìŠ¤ì¼ˆë ˆí†¤ ì¡°ì¢…ì„ì„ ì‚¬ìš©í•˜ëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œì½”-ë§¤ë‹ˆí“°ë ˆì´ì…˜ ì‹œìŠ¤í…œì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ë¡œë´‡ì˜ í˜•íƒœë¥¼ ë°˜ì˜í•˜ëŠ” ì¸ê°„ì´ ì°©ìš©í•˜ëŠ” ì—‘ì†ŒìŠ¤ì¼ˆë ˆí†¤ì„ í†µí•´ ì§ê´€ì ì¸ ì „ì‹  ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Isomorphic exoskeleton design
- Whole-body loco-manipulation
- Intuitive teleoperation interface
- Real-time motion capture and control

---

## AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control

**Authors**: Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, Xiaolong Wang

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: AMO introduces adaptive motion optimization for hyper-dexterous humanoid whole-body control. The system optimizes motion in real-time to achieve highly dexterous manipulation while maintaining whole-body balance and coordination.

**í•œê¸€**: AMOëŠ” ì´ˆì •ë°€ íœ´ë¨¸ë…¸ì´ë“œ ì „ì‹  ì œì–´ë¥¼ ìœ„í•œ ì ì‘í˜• ëª¨ì…˜ ìµœì í™”ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ì „ì‹  ê· í˜•ê³¼ í˜‘ì‘ì„ ìœ ì§€í•˜ë©´ì„œ ë§¤ìš° ì •ë°€í•œ ì¡°ì‘ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ì…˜ì„ ìµœì í™”í•©ë‹ˆë‹¤.

### Key Features
- Real-time motion optimization
- Hyper-dexterous manipulation capabilities
- Adaptive whole-body control
- Balance maintenance during manipulation

---

## Unleashing Humanoid Reaching Potential via Real-world-Ready Skill Space

**Authors**: Zhikai Zhang, Chao Chen, Han Xue, Jilong Wang, Sikai Liang, Yun Liu, Zongzhang Zhang, He Wang, Li Yi

**Year**: 2025

**Links**:
- ğŸ“„ arXiv: TBA
- ğŸŒ Project Page: TBA

### ìš”ì•½ (Summary)

**English**: This work presents a framework for unleashing humanoid reaching potential through a real-world-ready skill space. The system enables humanoids to perform diverse reaching tasks with high precision and adaptability.

**í•œê¸€**: ì´ ì—°êµ¬ëŠ” ì‹¤ì œ ì„¸ê³„ ì¤€ë¹„ê°€ ëœ ê¸°ìˆ  ê³µê°„ì„ í†µí•´ íœ´ë¨¸ë…¸ì´ë“œì˜ ë„ë‹¬ ì ì¬ë ¥ì„ ë°œíœ˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. ì‹œìŠ¤í…œì€ íœ´ë¨¸ë…¸ì´ë“œê°€ ë†’ì€ ì •ë°€ë„ì™€ ì ì‘ì„±ìœ¼ë¡œ ë‹¤ì–‘í•œ ë„ë‹¬ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

### Key Features
- Real-world-ready skill space
- Diverse reaching capabilities
- High-precision manipulation
- Adaptive task execution

---

[Back to Main README](../README.md)
